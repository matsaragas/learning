<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Learning by matsaragas</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <script type="text/javascript" src="LaTeXMathML.js"></script>
    <link rel="stylesheet" type="text/css" href="LaTeXMathML.standardarticle.css" /> 
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Learning</h1>
      <h2 class="project-tagline">Recurrent Neural Networks</h2>
      <a href="https://github.com/matsaragas/learning" class="btn">View on GitHub</a>
      <a href="https://github.com/matsaragas/learning/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/matsaragas/learning/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">

  <div class="LaTeX">
    \documentclass[12pt]{article}
    \begin{document}

    \title{Recurrent Neural Networks Theory and Applications}
    \maketitle

    \begin{abstract}\\
    
    In this document I will analyze the following:
    
    \begin{enumerate}
    \item How and why Recurrent Neural Networks (RNNs) works.
    \item What problems can we solve using RNN.
    \item Training RNNs
    \item Tricks for improving RNNs and latest developments.
    \end{enumerate}
    \end{abstract}

    \section{Recurrent Neural Networks}
    
    During the last five years Recurrent Neural Netwo
    rks have shown great promise in solving many challenging 
    problems involving sequential time-series data such as action recognition [1,2], multilingual machine translation [3,4] and 
    multimodal translation between videos and sentences [5]. 

    The structure of RNN is similar to that of a simple feed forward neural net, but differs in the architecture of how these neurons are connected to one another. RNNs allow connections among hidden units associated with a time delay. Through these connections the model can retain information about the past inputs, enabling it to discover temporal correlations between events that are possibly far away from each other in the data. RNNs are similar to the human brain, which is a large feedback network of connected neurons that somehow can learn to translate a lifelong sensory input stream into a sequence of useful motor outputs. \\ 

    
    Most traditional machine learning methods, however are much more limited than RNN. 
    In particular, unlike popular artificial feedforward neural networks (FNN) and Support 
    Vector Machines (SVM), RNN cannot only deal with stationary input and output patterns but 
    also with pattern sequences of arbitrary length. In fact, while FNN and SVM have been 
    extremely successful in restricted applications, they assume that all their inputs are 
    stationary and independent of each other. In the real world this is unrealistic: normally 
    past events influence future events. A temporary memory of things that happened a while ago 
    may be essential for producing a useful output action later. RNN can implement arbitrary 
    types of such short-term memories of internal states by means of their recurrent connections; 
    FNN and SVM cannot. In fact RNN can implement real sequence-processing and sequence producing 
    programs with loops and temporary variables, while FNN and SVM are limited to simple 
    feedforard mappings from inputs to outputs. Therefore RNN can solve many tasks unsolvable by 
    FNN and SVM. RNN are also more powerful than widely used probabilistic sequence processors
    such as Hidden Markov Models (HMM), which are unable to compactly encode complex memories of previous events.     

    Types of tasks for which RNNs can, in pronciple, be used:

    \begin{itemize}
    \item system identification and inverse system identification
    \item filtering and prediction
    \item pattern classification
    \item stochastic sequence modeling
    \item associateive memory
    \item data compression
    \end{itemize}

    Some relative application areas:

    \begin{itemize}
    \item telecommunication
    \item control and chemical plants
    \item control of engines and generators
    \item fault monitoring, biomedical diagnostics and monitoring
    \item speech recognition
    \item video data analysis
    \item man-machine interfaces
    \end{itemize}

    \section{Training RNN}
    
    In the forward pass of the RNN we have:
    
    \begin{equation}
    s_{t} = tanh(W_{rec} s_{t-1} + Ux_{t})
    \end{equation}
    
    For the prediction of each output in the sequence we have:
    
    \begin{equation}
    p({\hat y}_{t} = y_{j}) = \frac{e^{s_{t}v_{j}}}{\sum_{k=1}^{K} s_{t} v_{k}},
    \end{equation}
    
    where $j = 1, \hdots, K$. $K$ is the number of possible outputs that the ${\hat y}_{t}$ can take.
    

    
    <figure>
       <center><img src="images/rnn_unfold.pdf" alt="rnn unfold" width="650" height="328">
       <figcaption>Figure 1. A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Original by <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/">WILDML</a></figcaption>
       </center>
    </figure>
    

    
    \section{References}
    
    \begin{enumerate}
    \item Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term 
     recurrent  convolutional  networks  for  visual  recognition  and  description. arXiv preprint arXiv:1411.4389, 2014.
     \item A. Grushin, D. D. Monner, J. A. Reggia, and A. Mishra. Robust human action recognition via long short-term memory. 
     In Neural Networks (IJCNN).
     \item I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks.  In Advances in 
     Neural Information Processing Systems, pages 3104â€“3112, 2014.
     \item D.  Bahdanau,  K.  Cho,  and  Y.  Bengio. Neural  machine translation by jointly learning to align and 
     translate. arXiv preprint arXiv:1409.0473, 2014.
     \item S. Venugopalan,  H. Xu, J. Donahue,  M. Rohrbach, R. Mooney, and K. Saenko. Translating videos to natural 
     language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, 2014.
    \end{enumerate}

    \end{document}
  </div>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/matsaragas/learning">Learning</a> is maintained by <a href="https://github.com/matsaragas">matsaragas</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
